{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport cv2\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\ndata = []\nDataDir = '/kaggle/input/cat-and-dog/training_set/training_set'\ntypes = ['cats','dogs']\nimg_size = 50\nfor type in types:\n    path = os.path.join(DataDir,type)\n    type_num = types.index(type)\n    for img in os.listdir(path):\n        try:\n            img_array = cv2.imread(os.path.join(path,img))\n            new_array = cv2.resize(img_array,(img_size,img_size))\n            data.append([new_array,type_num])\n        except:\n            print(\"bad file\")\n\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = np.array(data)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.shape\nt = data.shape[0]\ntraining_data,testing_data = data[:int(3*t/4)],data[int(t/4):]","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:07:41.123551Z","iopub.execute_input":"2021-09-28T12:07:41.123869Z","iopub.status.idle":"2021-09-28T12:07:41.132047Z","shell.execute_reply.started":"2021-09-28T12:07:41.123835Z","shell.execute_reply":"2021-09-28T12:07:41.131163Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"x_train = []\ny_train = []\nfor i in range(training_data.shape[0]):\n    x_train.append(training_data[i][0])\n    y_train.append(training_data[i][1])\n\nx_train = np.array(x_train)\ny_train = np.array(y_train).reshape(-1,1).T\nprint(f'x_train = {x_train.shape}\\ny_train = {y_train.shape}')","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:17:02.615024Z","iopub.execute_input":"2021-09-28T12:17:02.615336Z","iopub.status.idle":"2021-09-28T12:17:02.655232Z","shell.execute_reply.started":"2021-09-28T12:17:02.615305Z","shell.execute_reply":"2021-09-28T12:17:02.654352Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"plt.imshow(x_train[4])","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:17:13.367871Z","iopub.execute_input":"2021-09-28T12:17:13.368210Z","iopub.status.idle":"2021-09-28T12:17:13.571557Z","shell.execute_reply.started":"2021-09-28T12:17:13.368176Z","shell.execute_reply":"2021-09-28T12:17:13.570891Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"x_train = x_train.reshape(-1,img_size*img_size*3).T\nx_train.shape\n","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:20:15.133906Z","iopub.execute_input":"2021-09-28T12:20:15.134202Z","iopub.status.idle":"2021-09-28T12:20:15.141058Z","shell.execute_reply.started":"2021-09-28T12:20:15.134172Z","shell.execute_reply":"2021-09-28T12:20:15.140182Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"def sigmoid(Z):\n    return 1/(1+np.exp(-Z))\n\ndef ReLU(Z):\n    return np.maximum(Z,0)","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:20:17.890921Z","iopub.execute_input":"2021-09-28T12:20:17.891730Z","iopub.status.idle":"2021-09-28T12:20:17.898285Z","shell.execute_reply.started":"2021-09-28T12:20:17.891694Z","shell.execute_reply":"2021-09-28T12:20:17.897293Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"layer_dims = [x_train.shape[0],20,5,1]\ndef initialize_parameters_deep(layer_dims):\n    params = []\n    for i in range(1,len(layer_dims)):\n        W =  np.random.randn(layer_dims[i],layer_dims[i-1])*0.01\n        b = np.zeros((layer_dims[i],1))*0.01\n\n        dic = {\n            \"W\":W,\n            \"b\":b\n        }\n        params.append(dic)\n    #eg: params = [W1,b1,W2,b2....]\n    # print(f'params = {params}')\n    return params\n\ndef linear_activation_forward(A_prev,W,b, activation):\n    \n    print(f'shape of W,A_prev = {W.shape}, {A_prev.shape}')\n\n    Z = np.dot(W,A_prev) + b\n\n    print(f'shape of Z = {Z.shape}')\n    \n    cache = (A_prev,W,b)\n    if activation == \"sigmoid\":\n        A = sigmoid(Z)\n\n    elif activation == \"relu\":\n        A = ReLU(Z)\n\n    return A, cache \n\ndef L_model_forward(X,params):\n    #size of layer\n    L = len(params)-1\n    caches = []\n    A = X\n\n    print(f'length of params = {L}')\n    for l in range(0,L):\n        A_prev = A\n        W,b = params[l][\"W\"],params[l][\"b\"]\n        A,cache = linear_activation_forward(A_prev,W,b,\"relu\")\n        caches.append(cache)\n\n    AL, cache = linear_activation_forward(A,params[L][\"W\"],params[L][\"b\"],\"sigmoid\")\n    caches.append(cache)\n\n    return AL, caches\n\n\ndef compute_cost(AL,Y):\n    m = Y.shape[1]\n    \n    print(f'shape of AL,Y = {AL.shape}, {Y.shape}')\n    cost = -1/m * np.sum(np.multiply(Y,np.log(AL)) + np.multiply(1-Y,np.log(1-AL)))\n    cost = np.squeeze(cost)\n\n    return cost","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:20:19.253900Z","iopub.execute_input":"2021-09-28T12:20:19.254493Z","iopub.status.idle":"2021-09-28T12:20:19.268984Z","shell.execute_reply.started":"2021-09-28T12:20:19.254456Z","shell.execute_reply":"2021-09-28T12:20:19.267981Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"#backward prop\ndef relu_backward(dZ_prev, cache):\n    A,b,b = cache\n    A[A>=0] = 1\n    A[A<0] = 0\n\n    dZ = np.multiply(dZ_prev,A)\n    return dZ\n\ndef sigmoid_backwards(dZ_prev, cache):\n    dZ = np.multiply(dZ_prev,(1-dZ_prev))\n    return dZ\n\ndef L_activation_backward(dA, cache, activation):\n    A_prev, W, b = cache\n    m = A_prev.shape[1]\n\n    if activation == \"relu\":\n        dZ = relu_backward(dA, cache)\n    elif activation == \"sigmoid\":\n        dZ = sigmoid_backwards(dA,cache)\n\n    dW = 1/m* np.dot(dZ,A_prev.T)\n    db = 1/m* np.sum(dZ, axis=1, keepdims=True)\n    dA_prev = np.dot(W.T,dZ)\n\n    return dA_prev, dW, db\n\ndef L_model_backward(AL, Y, caches):\n    grads = {}\n    L = len(caches)\n    m = AL.shape[1]\n    # Y = Y.reshape(AL.shape) \n    \n    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n    current_cache = caches[L-1]\n    dA_prev_temp, dW_temp, db_temp = L_activation_backward(dAL,current_cache,\"sigmoid\")\n\n    grads[\"dA\"+str(L-1)] = dA_prev_temp\n    grads[\"dW\"+ str(L)] = dW_temp\n    grads[\"db\"+ str(L)] = db_temp\n\n    for l in reversed(range(L-1)):\n        current_cache = caches[l]\n        dA_prev_temp, dW_temp, db_temp = L_activation_backward(dA_prev_temp,current_cache,\"relu\")\n\n        grads[\"dA\"+str(l)] = dA_prev_temp\n        grads[\"dW\"+str(l+1)] =  dW_temp\n        grads[\"db\"+str(l+1)] = db_temp\n    \n    return grads\n\ndef update_parameters(params, grads, learning_rate):\n    parameters = params.copy()\n    L = len(parameters)//2\n    \n    for l in range(L):\n        parameters[l][\"W\"] = parameters[l][\"W\"] - learning_rate*grads[\"dW\"+str(l)]\n        parameters[l][\"b\"] = parameters[l][\"b\"] - learning_rate*grads[\"db\"+str(l)]\n\n    return parameters\n        ","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:22:24.470823Z","iopub.execute_input":"2021-09-28T12:22:24.471862Z","iopub.status.idle":"2021-09-28T12:22:24.487943Z","shell.execute_reply.started":"2021-09-28T12:22:24.471822Z","shell.execute_reply":"2021-09-28T12:22:24.487157Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n\n    np.random.seed(1)\n    costs = []                         # keep track of cost\n    \n    parameters = initialize_parameters_deep(layers_dims)\n    \n    for i in range(0, num_iterations):\n        \n        AL, caches = L_model_forward(X, parameters)\n        \n        cost = compute_cost(AL,Y)\n        print(cost)\n\n        grads = L_model_backward(AL,Y,caches)\n        \n        # UPDATE parameters\n        parameters = update_parameters(parameters, grads, learning_rate)\n        \n                \n        # Print the cost every 100 iterations\n        # if print_cost and i % 1 == 0 or i == num_iterations - 1:\n        print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n        if i % 100 == 0 or i == num_iterations:\n            costs.append(cost)\n    \n    return parameters, costs","metadata":{"execution":{"iopub.status.busy":"2021-09-28T12:22:26.311718Z","iopub.execute_input":"2021-09-28T12:22:26.312120Z","iopub.status.idle":"2021-09-28T12:22:26.320775Z","shell.execute_reply.started":"2021-09-28T12:22:26.312076Z","shell.execute_reply":"2021-09-28T12:22:26.319806Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"code","source":"parameters,costs = L_layer_model(x_train,y_train,layer_dims,num_iterations=100,print_cost=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}